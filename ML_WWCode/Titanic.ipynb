{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will build a classification model using DecisionTrees and Random forest classifier from python's scikit learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. Data Loading\n",
    "2. Data Exploration\n",
    "3. Visualization\n",
    "4. Preprocessing\n",
    "5. Decision Trees and hyperparameter analysis \n",
    "5. Random Forest\n",
    "6. Model comparision using ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will import all the necessary packages and load the datasets we plan to work on. We will use the \n",
    "<a href='https://www.kaggle.com/jessemostipak/hotel-booking-demand'> Hotel booking data </a> and build a model to determine which customers will cancel their hotel booking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix,auc,roc_auc_score,roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# file_path = 'C:\\Users\\Tejal\\Documents\\Tejal\\WWC-siliconvalley\\hotel_bookings.csv'\n",
    "# ALREADY MADE:\n",
    "# file_path='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\n",
    "# df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the data, its features and distribution is a major part of builiding ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataset\n",
    "# df.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatype of features\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature list \n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "# percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "# missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "#                                  'percent_missing': percent_missing})\n",
    "# missing_value_df.sort_values('percent_missing', ascending=False,inplace=True)\n",
    "# missing_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Company, agent, country and children have null values. There are multiple techniques for imputing null value but for simplicity we impute them with 0. As company has a very high null value percentage we will drop the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create a copy of dataframe for backup and impute null with 0\n",
    "# backup_df=df.copy\n",
    "# df = df.drop('company',axis=1)\n",
    "# df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The df has no Null values\n",
    "# (df['agent'].isnull().sum()/len(df)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, our target variable is is_cancelled which indicates if the booking was cancelled. 1 --> canceled, 0 --> Not canceled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['is_canceled'].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37% customers have cancelled their bookings. we see that our data in imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotel feature count and distribution across 0 and 1 class \n",
    "# df['hotel'].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.countplot(x='is_canceled',hue='hotel',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data has higher city hotel reservation data points compared to resort, above observation is on par with  same trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market segments\n",
    "# df.groupby(['market_segment'])['is_canceled'].count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derive new features using existing features\n",
    "2. Remove irrelevant features\n",
    "3. Transform existing features\n",
    "4. Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into train validation & test set in train:val:test=60:20:20 size\n",
    "# # We are splitting the data into 3 chunks as we will be tuning many hyperparameters in this notebook\n",
    "# train, val_test = train_test_split(df, test_size=0.4, random_state = 42)\n",
    "# val, test = train_test_split(val_test, test_size=0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path='https://raw.githubusercontent.com/WomenWhoCode/WWCodeDataScience/master/Intro_to_MachineLearning/data/titanic/test.csv'\n",
    "test = pd.read_csv(test_file_path)\n",
    "train_file_path='https://raw.githubusercontent.com/WomenWhoCode/WWCodeDataScience/master/Intro_to_MachineLearning/data/titanic/train.csv'\n",
    "train = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])  # stackoverflow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418, 11)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23878, 31)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stays_in_week_nights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/WWC-IntroToML/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stays_in_week_nights'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-988a6eef5d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Let us add weekend stay and weekday stay days to get total days of stay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_days'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_week_nights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_weekend_nights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_days'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_week_nights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_weekend_nights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_days'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_week_nights'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stays_in_weekend_nights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/WWC-IntroToML/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/WWC-IntroToML/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stays_in_week_nights'"
     ]
    }
   ],
   "source": [
    "# #Let us add weekend stay and weekday stay days to get total days of stay\n",
    "# train['total_days'] = train['stays_in_week_nights'] + train['stays_in_weekend_nights']\n",
    "# test['total_days'] = test['stays_in_week_nights'] + test['stays_in_weekend_nights']\n",
    "# val['total_days'] = val['stays_in_week_nights'] + val['stays_in_weekend_nights']\n",
    "\n",
    "# # drop the weekend stay and weekday stay days features\n",
    "# train = train.drop('stays_in_week_nights',axis=1).drop('stays_in_weekend_nights',axis=1)\n",
    "# test = test.drop('stays_in_week_nights',axis=1).drop('stays_in_weekend_nights',axis=1)\n",
    "# val = val.drop('stays_in_week_nights',axis=1).drop('stays_in_weekend_nights',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_0=train[(train['is_canceled']==0)]\n",
    "# train_1=train[train['is_canceled']==1]\n",
    "# sns.set(rc={\"figure.figsize\": (20, 20)})\n",
    "# subplot(2,2,1)\n",
    "# ax = sns.distplot(train_0['total_days'], bins=100, color='r')\n",
    "# subplot(2,2,2)\n",
    "# ax=sns.distplot(train_1['total_days'], bins=100, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Total customers\n",
    "# train['total_customers'] = train['adults'] + train['children']+train['babies']\n",
    "# test['total_customers'] = test['adults'] + test['children']+test['babies']\n",
    "# val['total_customers'] = val['adults'] + val['children']+val['babies']\n",
    "\n",
    "\n",
    "# train = train.drop('adults',axis=1).drop('children',axis=1).drop('babies',axis=1)\n",
    "# test = test.drop('adults',axis=1).drop('children',axis=1).drop('babies',axis=1)\n",
    "# val = val.drop('adults',axis=1).drop('children',axis=1).drop('babies',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['total_customers'].value_counts().plot(kind='bar',figsize=(5,5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['reservation_status_date'],axis=1)\n",
    "test = test.drop(['reservation_status_date'],axis=1)\n",
    "val = val.drop(['reservation_status_date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(train['agent'].unique())) # 309 unique values - Large number of unique agents and it is categorical, difficult to encode\n",
    "train = train.drop('agent',axis=1)\n",
    "test = test.drop('agent',axis=1)\n",
    "val = val.drop('agent',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train['country'].unique())) # 160 countries\n",
    "train = train.drop('country',axis=1)\n",
    "test = test.drop('country',axis=1)\n",
    "val = val.drop('country',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(column='previous_bookings_not_canceled',bins=20,figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['previous_bookings_not_canceled'].value_counts() # We observe that most data has value = 0; hence we drop the feature\n",
    "#train.groupby(['is_canceled'])['previous_bookings_not_canceled'].value_counts() # We observe that data distribution across both class is remains same\n",
    "train = train.drop('previous_bookings_not_canceled',axis=1)\n",
    "test = test.drop('previous_bookings_not_canceled',axis=1)\n",
    "val = val.drop('previous_bookings_not_canceled',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['is_canceled'])['previous_cancellations'].value_counts().plot(kind='bar',figsize=(5,5))\n",
    "# We observe that most data has value = 0; and trend remains same across the 2 classes\n",
    "train = train.drop('previous_cancellations',axis=1)\n",
    "test = test.drop('previous_cancellations',axis=1)\n",
    "val = val.drop('previous_cancellations',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train = train.copy()\n",
    "backup_test = test.copy()\n",
    "backup_val = val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom encoding\n",
    "train['arrival_date_month'] = train['arrival_date_month'].map({'January':1, 'February': 2, 'March':3, \\\n",
    "                                                         'April':4, 'May':5, 'June':6, 'July':7,\\\n",
    "                                                         'August':8, 'September':9, 'October':10, \\\n",
    "                                                         'November':11, 'December':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['market_segment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=['hotel','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type','reservation_status']\n",
    "for i in cat_col:\n",
    "    train[i] = encode.fit_transform(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['market_segment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.head()\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation\n",
    "<b>Spearman</b> and <b>Pearson</b> are the 2 statistical methods to compute the correlation between features. \n",
    "- Pearson is suggested method for features with continuous values and linear relationship\n",
    "- Spearman is suggested method when features have ordinal categorical data or non-linear relationship\n",
    "<br>Pandas correlation method by default uses Pearson method, but we can also change it to spearman </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_corr = train.corr()\n",
    "feat_corr['is_repeated_guest'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(feat_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal shows correlation of each feature with itself, hence indicates highest correlation.\n",
    "Using the table and plot we observe that few features have veryhigh correlation\n",
    "Ex:- \n",
    "1. Arrival_date_week_number and arrival_date_month = 0.99\n",
    "2. reserved_room_type vs assigned_room_type = 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_corr['is_canceled'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservation_status has high correlation with is_canceled. In Naive Bayes session, we saw that removing the reservation_status feature caused the model performance to drop considerably. Lets see how it affects Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various decision tree algorithms like - ID3, C4.5, C5.0 and CART. Scikit learn implements optimized version of CART alogrithm. We have multiple hyperparameters in decision tree, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">sklearn documentation</a> \n",
    "<br> <br>\n",
    "We will try to see the effect of following hyperparameters on modelling -\n",
    "1. criterion -{gini and entropy}\n",
    "2. max_depth\n",
    "3. class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 -\n",
    "Default hyperparaments -- Gini criterion, no class weight and no pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = backup_train[\"is_canceled\"]\n",
    "X_train = backup_train.drop([\"is_canceled\"], axis=1)\n",
    "y_val = backup_val[\"is_canceled\"]\n",
    "X_val = backup_val.drop([\"is_canceled\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit's Decision tree and Random Forest implementations cannot handle string values so we need to encode the categorical values to convert them to numeric value. However, few other languages like R, Spark and Weka have Decision trees that can handle string feature values <br><br>\n",
    "We use one-hot encoding instead of label encoder to avoid the illusion of continuous values for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type','reservation_status']\n",
    "X_train_enc = pd.get_dummies(data=X_train,columns=cat_cols)\n",
    "X_val_enc = pd.get_dummies(data=X_val,columns=cat_cols)\n",
    "X_train_enc,X_val_enc =X_train_enc.align(X_val_enc, join='left', axis=1)\n",
    "X_val_enc=X_val_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 0)\n",
    "clf.fit(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val_enc)\n",
    "y_prob = clf.predict_proba(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Precission and Recall </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PR diagram1.PNG\" width=\"200p\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/PR diagram 2.PNG\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b> Confusion Matrix </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Confusion matrix.PNG\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test-set confusion matrix:\\n', confusion_matrix(y_val,y_pred)) \n",
    "print(\"recall score: \", recall_score(y_val,y_pred))\n",
    "print(\"precision score: \", precision_score(y_val,y_pred))\n",
    "print(\"f1 score: \", f1_score(y_val,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(\n",
    "    {'Features': list(X_train_enc.columns),\n",
    "     'Importance': clf.feature_importances_\n",
    "    })\n",
    "d.sort_values(by=['Importance'],ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance is perfect but only 1 feature has been used in the model, hence we should remove that feature to avoid data leak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - \n",
    "Remove the feature that is highly correlated with target feature\n",
    "<br>\n",
    "<b>Reservation_status</b> has high correlation with is_canceled. Looking at the values in column reveals that canceled is a reservation type. This might be causing data leak. Hence we will delete this feature and train with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reservation_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('reservation_status',axis=1)\n",
    "X_val = X_val.drop('reservation_status',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_train_enc = pd.get_dummies(data=X_train,columns=cat_cols,drop_first=True)\n",
    "X_val_enc = pd.get_dummies(data=X_val,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_val_enc =X_train_enc.align(X_val_enc, join='left', axis=1)\n",
    "X_val_enc=X_val_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(random_state = 0)\n",
    "clf2.fit(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = clf2.predict(X_val_enc)\n",
    "y_prob2 = clf2.predict_proba(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test-set confusion matrix:\\n', confusion_matrix(y_val,y_pred2)) \n",
    "print(\"recall score: \", recall_score(y_val,y_pred2))\n",
    "print(\"precision score: \", precision_score(y_val,y_pred2))\n",
    "print(\"f1 score: \", f1_score(y_val,y_pred2))\n",
    "print(\"accuracy score: \", accuracy_score(y_val,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(\n",
    "    {'Features': list(X_train_enc.columns),\n",
    "     'Importance': clf2.feature_importances_\n",
    "    })\n",
    "d.sort_values(by=['Importance'],ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "Let us remove 1 feature from the correlated feature pair, We will remove the feature with lesser importance\n",
    "1. Arrival_date_week_number and arrival_date_month = 0.99\n",
    "2. reserved_room_type vs assigned_room_type = 0.81\n",
    "3. market_segment vs distribution_channel = 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('arrival_date_month',axis=1)\n",
    "X_val = X_val.drop('arrival_date_month',axis=1)\n",
    "\n",
    "X_train = X_train.drop('market_segment',axis=1)\n",
    "X_val = X_val.drop('market_segment',axis=1)\n",
    "\n",
    "X_train = X_train.drop('reserved_room_type',axis=1)\n",
    "X_val = X_val.drop('reserved_room_type',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['hotel','arrival_date_year','meal','distribution_channel','assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_train_enc = pd.get_dummies(data=X_train,columns=cat_cols,drop_first=True)\n",
    "X_val_enc = pd.get_dummies(data=X_val,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_val_enc =X_train_enc.align(X_val_enc, join='left', axis=1)\n",
    "X_val_enc=X_val_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = DecisionTreeClassifier(random_state = 0)\n",
    "clf3.fit(X_train_enc, y_train)\n",
    "y_pred3=clf3.predict(X_val_enc)\n",
    "print(\"f1 score: \", f1_score(y_val,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3_train = clf3.predict(X_train_enc)\n",
    "print('test-set confusion matrix:\\n', confusion_matrix(y_train,y_pred3_train)) \n",
    "print(\"f1 score: \", f1_score(y_train,y_pred3_train))\n",
    "print(\"accuracy score: \", accuracy_score(y_train,y_pred3_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has very low training error but considerably high test error. This indicates that the model is overfitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(\n",
    "    {'Features': list(X_train_enc.columns),\n",
    "     'Importance': clf3.feature_importances_\n",
    "    })\n",
    "d.sort_values(by=['Importance'],ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - \n",
    "$Gini Impurity= \\sum_{k=1}^{c} (P_k)*(1-P_k)$ <br><br>\n",
    "$Entropy = \\sum_{k=1}^{c}-P_k*log_2(P_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Impurity criterion.PNG\" width=\"400\"/>\n",
    "We see that both criterion follow same curve indicating that there is no significant difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = backup_train[\"is_canceled\"]\n",
    "X_train = backup_train.drop([\"is_canceled\"], axis=1).drop([\"reservation_status\"],axis=1)\n",
    "y_val = backup_val[\"is_canceled\"]\n",
    "X_val = backup_val.drop([\"is_canceled\"], axis=1).drop([\"reservation_status\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_train_enc = pd.get_dummies(data=X_train,columns=cat_cols,drop_first=True)\n",
    "X_val_enc = pd.get_dummies(data=X_val,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_val_enc =X_train_enc.align(X_val_enc, join='left', axis=1)\n",
    "X_val_enc=X_val_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = DecisionTreeClassifier(criterion=\"entropy\",random_state = 0)\n",
    "clf4.fit(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = clf4.predict(X_val_enc)\n",
    "y_prob4 = clf4.predict_proba(X_val_enc)\n",
    "print(\"f1 score: \", f1_score(y_val,y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impurity criterion did not affect our model performance. The feature importance of the 2 models also look similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(\n",
    "    {'Features': list(X_train_enc.columns),\n",
    "     'Clf2_Importance': clf2.feature_importances_,\n",
    "     'Clf4_Importance': clf4.feature_importances_\n",
    "\n",
    "    })\n",
    "d.sort_values(by=['Clf4_Importance'],ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 -\n",
    "Pruning the model to avoid overfitting <br>\n",
    "We have multiple hyperparameters such as max_depth, min_sample_split etc which can be tuned to prune the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "   clf5 = DecisionTreeClassifier(max_depth=max_depth)\n",
    "   clf5.fit(X_train_enc, y_train)\n",
    "   train_pred = clf5.predict(X_train_enc)\n",
    "   f1_score1 = f1_score(y_train,train_pred)\n",
    "   train_results.append(f1_score1)\n",
    "\n",
    "   y_pred = clf5.predict(X_val_enc)\n",
    "   f1_score1 = f1_score(y_val,y_pred)\n",
    "   test_results.append(f1_score1)\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train F1-score\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Val F1-score\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('F1-score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the tree depth increases, our training f1-score improves and we eventually get f1-score=1, however the widening gap between the test and training error curve indicates that the model is unable to generalize well on unseen data i.e. the model has overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = DecisionTreeClassifier(criterion=\"gini\",random_state = 0,max_depth=12)\n",
    "clf5.fit(X_train_enc, y_train)\n",
    "y_pred5 = clf5.predict(X_val_enc)\n",
    "y_pred5_train = clf5.predict(X_train_enc)\n",
    "print(\"Train data f1 score: \", f1_score(y_train,y_pred5_train))\n",
    "print(\"Val data f1 score: \", f1_score(y_val,y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6\n",
    "<b> Weighted Decision tree or Cost-sensitive tree </b> <br>\n",
    "Experiment with class weight. Our data is slightly imbalanced, so try to assign higher weight for positive samples versus negative samples using <b>class_weight</b> hyeperparameter. We can assign {class:weight} or “balanced”<br>\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6 = DecisionTreeClassifier(criterion=\"gini\",random_state = 0,max_depth=12,class_weight={0:1,1:2})\n",
    "clf6.fit(X_train_enc, y_train)\n",
    "y_pred6 = clf6.predict(X_val_enc)\n",
    "y_pred6_train = clf6.predict(X_train_enc)\n",
    "print(\"Train data f1 score: \", f1_score(y_train,y_pred6_train))\n",
    "print(\"Val data f1 score: \", f1_score(y_val,y_pred6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adjusting the class_weight, our test f1 score has improved by 1.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree has many hyperparameters and we can use sklearn's <b>GridSearchCV</b> or <b>RandomizedSearchCV</b> to find the best hyperparaters. <a href=\"https://scikit-learn.org/stable/modules/grid_search.html\">Sklearn documentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model - Random forest\n",
    "Random forest is a ensemble of decision trees which aims to improve prediction accuracy while avoiding over fitting. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn's Random forest implementation, each subsample used to fit each tree is same size as actual data but sampled using replacement if <b>bootstrap </b> hyperparameter is set to True. It has many hyperparameters similar to decision tree. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">sklearn documenatation </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = backup_train[\"is_canceled\"]\n",
    "X_train = backup_train.drop([\"is_canceled\"], axis=1).drop([\"reservation_status\"],axis=1)\n",
    "y_val = backup_val[\"is_canceled\"]\n",
    "X_val = backup_val.drop([\"is_canceled\"], axis=1).drop([\"reservation_status\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_train_enc = pd.get_dummies(data=X_train,columns=cat_cols,drop_first=True)\n",
    "X_val_enc = pd.get_dummies(data=X_val,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_val_enc =X_train_enc.align(X_val_enc, join='left', axis=1)\n",
    "X_val_enc=X_val_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1=RandomForestClassifier()\n",
    "rf1.fit(X_train_enc,y_train)\n",
    "y_pred_rf1=rf1.predict(X_val_enc)\n",
    "y_pred_rf1_train=rf1.predict(X_train_enc)\n",
    "\n",
    "print(\"Train data f1 score: \", f1_score(y_train,y_pred_rf1_train))\n",
    "print(\"Val data f1 score: \", f1_score(y_val,y_pred_rf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default setting, we see that the model has better performance on test data but the training data f1 score is very high, signifying a overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "1. Using optimal max_depth \n",
    "2. Class_weight = \"balanced_subsample\" - Same as balanced but the ratio is computed for each tree based on the bootstrapped data considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "   rf2=RandomForestClassifier(max_depth=max_depth,n_estimators=30)\n",
    "   rf2.fit(X_train_enc, y_train)\n",
    "   train_pred = rf2.predict(X_train_enc)\n",
    "   f1_score1 = f1_score(y_train,train_pred)\n",
    "   train_results.append(f1_score1)\n",
    "\n",
    "   y_pred = rf2.predict(X_val_enc)\n",
    "   f1_score1 = f1_score(y_val,y_pred)\n",
    "   test_results.append(f1_score1)\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train F1-score\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Val F1-score\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('F1-score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2=RandomForestClassifier(max_depth=15,class_weight=\"balanced_subsample\")\n",
    "rf2.fit(X_train_enc,y_train)\n",
    "y_pred_rf2=rf2.predict(X_val_enc)\n",
    "y_pred_rf2_train=rf2.predict(X_train_enc)\n",
    "\n",
    "print(\"Train data f1 score: \", f1_score(y_train,y_pred_rf2_train))\n",
    "print(\"Val data f1 score: \", f1_score(y_val,y_pred_rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "1. Increase number of estimators to 80; It increases the training time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3=RandomForestClassifier(max_depth=17,class_weight=\"balanced_subsample\",n_estimators=80)\n",
    "rf3.fit(X_train_enc,y_train)\n",
    "y_pred_rf3=rf3.predict(X_val_enc)\n",
    "y_pred_rf3_train=rf3.predict(X_train_enc)\n",
    "\n",
    "print(\"Train data f1 score: \", f1_score(y_train,y_pred_rf3_train))\n",
    "print(\"Val data f1 score: \", f1_score(y_val,y_pred_rf3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "We will compare the best versions of the 2 classifiers we have trained so far in classification session -\n",
    "1. Decision tree model \n",
    "2. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = backup_test[\"is_canceled\"]\n",
    "X_test = backup_test.drop([\"is_canceled\"], axis=1).drop('reservation_status',axis=1)\n",
    "\n",
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_test_enc = pd.get_dummies(data=X_test,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_test_enc =X_train_enc.align(X_test_enc, join='left', axis=1)\n",
    "X_test_enc=X_test_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob6 = clf6.predict_proba(X_test_enc)\n",
    "false_positive_rateDT_6, true_positive_rateDT_6, thresholdDT_6 = roc_curve(y_test, y_prob6[:,1])\n",
    "roc_aucDT_6 = auc(false_positive_rateDT_6, true_positive_rateDT_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = backup_test[\"is_canceled\"]\n",
    "X_test = backup_test.drop([\"is_canceled\"], axis=1).drop('reservation_status',axis=1)\n",
    "cat_cols=['hotel','arrival_date_month','arrival_date_year','meal','market_segment','distribution_channel','reserved_room_type', 'assigned_room_type',\\\n",
    "        'deposit_type','customer_type']\n",
    "X_test_enc = pd.get_dummies(data=X_test,columns=cat_cols,drop_first=True)\n",
    "X_train_enc,X_test_enc =X_train_enc.align(X_test_enc, join='left', axis=1)\n",
    "X_test_enc=X_test_enc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_rf = rf3.predict_proba(X_test_enc)\n",
    "false_positive_rateRF, true_positive_rateRF, thresholdRF = roc_curve(y_test, y_prob_rf[:,1])\n",
    "roc_aucRF = auc(false_positive_rateRF, true_positive_rateRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rateDT_6, true_positive_rateDT_6, color = 'red', label = 'DT AUC = %0.2f' % roc_aucDT_6)\n",
    "plt.plot(false_positive_rateRF, true_positive_rateRF, color = 'green', label = 'RF AUC = %0.2f' % roc_aucRF)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1], linestyle = '--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Random forest has the better performance among the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
