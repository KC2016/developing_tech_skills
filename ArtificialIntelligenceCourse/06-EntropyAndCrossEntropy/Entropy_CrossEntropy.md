<img align="right"  height="100" src="../images/lesson06.png">

#  Entropy and cross entropy - Artificial Intelligence Course
6th lesson of the Artificial Intelligence Crash Course for all<BR/>
By Diogo Cortiz (TIDD/PUC-SP)<BR/>

Theoretical class<BR/>
Bit or Shannon: unit for measurement, aleatory variable between 0 and 1.<BR/>
Information measure uncertainty of an event.<BR/>
I(x)=-logP(x)<BR/>
'>probability,  < amount of information' 
<BR/>
Entropy: quantify the amount of information in a variable.<BR/>
Cross entropy: same net of events with different probabilities.<BR/>
Hp,q = 	-\sum_P(x)*logQ(x)<BR/>
P(x)=yi<BR/>
yi=valor real
ˆyi-valor estimado
L(yi,ˆyi)=-\sum_1^n yi*logˆyi 

[video_at_youtube](https://www.youtube.com/watch?v=ciWT5r-ckpw&t=11s)